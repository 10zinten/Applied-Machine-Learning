{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch provides the elegantly designed modules and classes:\n",
    "- `torch.nn`, `torch.optim`, `Dataset`, and `DataLoader` to help you create and train neural networks\n",
    "- In n order to fully utilize their power and customize them for your problem, you need to really understand exactly what they're doing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST data setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import requests\n",
    "\n",
    "DATA_PATH = Path('data')\n",
    "PATH = DATA_PATH/'mnist'\n",
    "\n",
    "PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "URL='http://deeplearning.net/data/mnist/'\n",
    "FILENAME='mnist.pkl.gz'\n",
    "\n",
    "if not (PATH/FILENAME).exists():\n",
    "    content = requests.get(URL+FILENAME).content\n",
    "    (PATH/FILENAME).open('wb').write(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle, gzip\n",
    "\n",
    "with gzip.open(PATH/FILENAME, 'rb') as f:\n",
    "    ((x_train, y_train), (x_valid, y_valid), _) = pickle.load(f, encoding='latin-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((50000, 784), (50000,), (10000, 784), (10000,))"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape, y_train.shape, x_valid.shape, y_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "plt.imshow(x_train[0].reshape((28, 28)), cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch uses `torch.tensor`, rather than numpy arrays, so we need to convert our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]]),\n",
       " torch.Size([50000, 784]),\n",
       " tensor(0),\n",
       " tensor(9))"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x_train,y_train,x_valid,y_valid = map(torch.tensor, (x_train,y_train,x_valid,y_valid))\n",
    "n,c = x_train.shape\n",
    "x_train, x_train.shape, y_train.min(), y_train.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural net from scratch (no torch.nn)\n",
    "\n",
    "- For the weights, we set `requires_grad` after the initialization, since we don't want that step included in the gradient. (Note that a trailling `_` in PyTorch signifies that the operation is performed in-place.)\n",
    "- NB: We are initializing the weights here with `Xavier initialisation` (by multiplying with `1/sqrt(n)`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "weights = torch.randn(784,10)/math.sqrt(784)\n",
    "weights.requires_grad_()\n",
    "bias = torch.zeros(10, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_softmax(x): return x - x.exp().sum(-1).log().unsqueeze(-1)\n",
    "\n",
    "def model(xb): return log_softmax(xb @ weights + bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([-2.7307, -1.9816, -2.2988, -2.3469, -2.5004, -2.0978, -2.5529, -2.4600,\n",
       "         -1.9636, -2.3802], grad_fn=<SelectBackward>), torch.Size([64, 10]))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bs = 64\n",
    "\n",
    "xb = x_train[: bs]\n",
    "preds = model(xb)\n",
    "preds[0], preds.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you see, the `preds` tensor contains not only the tensor values, but also a gradient function. We'll use this later to do backprop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "negative log-likelihood to use as the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nll(inp, target): return -inp[range(target.shape[0]), target].mean()\n",
    "loss_fn = nll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.2690, grad_fn=<NegBackward>)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yb = y_train[: bs]\n",
    "loss_fn(preds, yb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(out, yb):\n",
    "    preds = torch.argmax(out, dim=1)\n",
    "    return (preds == yb).float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1562)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(preds, yb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now run a training loop. For each iteration, we will:\n",
    "- select a mini-batch of data (of size bs)\n",
    "- use the model to make predictions\n",
    "- calculate the loss\n",
    "- `loss.backward()` updates the gradients of the model, in this case, weights and bias.\n",
    "- We now use these gradients to update the weights and bias. We do this within the `torch.no_grad()` context manager, because we do not want these actions to be recorded for our next calculation of the gradient. You can read more about how PyTorch's Autograd records operations here.\n",
    "- We then set the gradients to zero, so that we are ready for the next loop. Otherwise, our gradients would record a running tally of all the operations that had happened (i.e. `loss.backward()` adds the gradients to whatever is already stored, rather than replacing them)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.debugger import set_trace\n",
    "\n",
    "lr = 0.5\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 - loss: 0.4097 - accuracy: 0.9375\n",
      "Epoch: 2 - loss: 0.3201 - accuracy: 0.9375\n",
      "Epoch: 3 - loss: 0.2749 - accuracy: 0.9375\n",
      "Epoch: 4 - loss: 0.2451 - accuracy: 0.9375\n",
      "Epoch: 5 - loss: 0.2235 - accuracy: 0.9375\n",
      "Epoch: 6 - loss: 0.2071 - accuracy: 0.9375\n",
      "Epoch: 7 - loss: 0.1943 - accuracy: 0.9375\n",
      "Epoch: 8 - loss: 0.1841 - accuracy: 0.9375\n",
      "Epoch: 9 - loss: 0.1759 - accuracy: 0.9375\n",
      "Epoch: 10 - loss: 0.1692 - accuracy: 0.9375\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    for i in range((n-1)//bs + 1):\n",
    "        # set_trace()\n",
    "        start_i = i * bs\n",
    "        end_i = start_i + bs\n",
    "        xb = x_train[start_i: end_i]\n",
    "        yb = y_train[start_i: end_i]\n",
    "        pred = model(xb)\n",
    "        loss = loss_fn(pred, yb)\n",
    "        acc = accuracy(pred, yb)\n",
    "        \n",
    "        loss.backward()\n",
    "        with torch.no_grad():\n",
    "            weights -= lr * weights.grad\n",
    "            bias -= lr * bias.grad\n",
    "            weights.grad.zero_()\n",
    "            bias.grad.zero_()\n",
    "            \n",
    "    print(f\"Epoch: {epoch+1} - loss: {loss:.4f} - accuracy: {acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.2742, grad_fn=<NegBackward>), tensor(0.9531))"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xb = x_valid[: bs]\n",
    "yb = y_valid[: bs]\n",
    "\n",
    "loss_fn(model(xb), yb), accuracy(model(xb), yb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([9, 3, 7, 0, 9, 0, 8, 5, 5, 2, 4, 5, 0, 8, 4, 8])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.argmax(model(xb), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([9, 3, 7, 0, 9, 0, 8, 5, 5, 2, 4, 5, 0, 8, 4, 8])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using torch.nn.functional\n",
    "- we'll start taking advantage of PyTorch's nn classes to make it more concise and flexible\n",
    "- The first and easiest step is to make our code shorter by replacing our hand-written activation and loss functions with those from `torch.nn.functional`(which is generally imported into the namespace F by convention)\n",
    "- This module contains all the functions in the torch.nn library (whereas other parts of the library contain classes)\n",
    "- Pytorch provides a single function `F.cross_entropy` that combines `log likelihood loss` and `log softmax` activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "loss_fn = F.cross_entropy\n",
    "\n",
    "def model(xb): return xb @ weights + bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.0557, grad_fn=<NllLossBackward>), tensor(1.))"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_fn(model(xb), yb), accuracy(model(xb), yb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Refactor using nn.Module\n",
    "- Next up, we'll use `nn.Module` and `nn.Parameter`, for a clearer and more concise training loop\n",
    "- We subclass `nn.Module` (which itself is a class and able to keep track of state) to holds our `weights`, `bias`, and method for the `forward step`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class Mnist_logistic(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.weights = nn.Parameter(torch.randn(784, 10) / math.sqrt(784))\n",
    "        self.bias = nn.Parameter(torch.zeros(10))\n",
    "    \n",
    "    def forward(self, xb):\n",
    "        return xb @ self.weights + self.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Mnist_logistic()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that `nn.Module` objects are used as if they are functions (i.e they are callable), but behind the scenes Pytorch will call our forward method automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.3817, grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_fn(model(xb), yb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit():\n",
    "    for epoch in range(epochs):\n",
    "        for i in range((n-1)//bs + 1):\n",
    "            start_i = i * bs\n",
    "            end_i = start_i + bs\n",
    "            xb = x_train[start_i: end_i]\n",
    "            yb = y_train[start_i: end_i]\n",
    "            pred = model(xb)\n",
    "            loss = loss_fn(pred, yb)\n",
    "            acc = accuracy(pred, yb)\n",
    "\n",
    "            loss.backward()\n",
    "            with torch.no_grad():\n",
    "                for p in model.parameters():\n",
    "                    p -= lr * p.grad\n",
    "                model.zero_grad()\n",
    "        print(f\"Epoch: {epoch+1} - loss: {loss:.4f} - accuracy: {acc:.4f}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 - loss: 0.1595 - accuracy: 0.9375\n",
      "Epoch: 2 - loss: 0.1552 - accuracy: 0.9375\n",
      "Epoch: 3 - loss: 0.1516 - accuracy: 0.9375\n",
      "Epoch: 4 - loss: 0.1484 - accuracy: 0.9375\n",
      "Epoch: 5 - loss: 0.1457 - accuracy: 0.9375\n",
      "Epoch: 6 - loss: 0.1432 - accuracy: 0.9375\n",
      "Epoch: 7 - loss: 0.1410 - accuracy: 0.9375\n",
      "Epoch: 8 - loss: 0.1391 - accuracy: 0.9375\n",
      "Epoch: 9 - loss: 0.1373 - accuracy: 0.9375\n",
      "Epoch: 10 - loss: 0.1356 - accuracy: 0.9375\n"
     ]
    }
   ],
   "source": [
    "fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Refactor using nn.Linear\n",
    "- Instead of manually defining and initializing `self.weights` and `self.bias`, and calculating `xb  @ self.weights + self.bias`, we will instead use the Pytorch class `nn.Linear` for a linear layer, which does all that for us\n",
    "- Pytorch has many types of `predefined layers` that can `greatly simplify our code`, and often makes it `faster too`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mnist_Logistic(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.lin = nn.Linear(784, 10)\n",
    "        \n",
    "    def forward(self, xb):\n",
    "        return self.lin(xb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.2884, grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Mnist_Logistic()\n",
    "loss_fn(model(xb), yb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 - loss: 0.4044 - accuracy: 0.9375\n",
      "Epoch: 2 - loss: 0.3162 - accuracy: 0.9375\n",
      "Epoch: 3 - loss: 0.2719 - accuracy: 0.9375\n",
      "Epoch: 4 - loss: 0.2427 - accuracy: 0.9375\n",
      "Epoch: 5 - loss: 0.2215 - accuracy: 0.9375\n",
      "Epoch: 6 - loss: 0.2055 - accuracy: 0.9375\n",
      "Epoch: 7 - loss: 0.1929 - accuracy: 0.9375\n",
      "Epoch: 8 - loss: 0.1830 - accuracy: 0.9375\n",
      "Epoch: 9 - loss: 0.1750 - accuracy: 0.9375\n",
      "Epoch: 10 - loss: 0.1684 - accuracy: 0.9375\n"
     ]
    }
   ],
   "source": [
    "fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Refactor using optim\n",
    "- Pytorch also has a package with various optimization algorithms, `torch.optim`\n",
    "- We can use the step method from our optimizer to take a forward step, instead of manually updating each parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "\n",
    "def get_model():\n",
    "    model = Mnist_Logistic()\n",
    "    return model, optim.SGD(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.2495, grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model, opt = get_model()\n",
    "loss_fn(model(xb), yb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 - loss: 0.4006 - accuracy: 0.9375\n",
      "Epoch: 2 - loss: 0.3129 - accuracy: 0.9375\n",
      "Epoch: 3 - loss: 0.2688 - accuracy: 0.9375\n",
      "Epoch: 4 - loss: 0.2399 - accuracy: 0.9375\n",
      "Epoch: 5 - loss: 0.2189 - accuracy: 0.9375\n",
      "Epoch: 6 - loss: 0.2030 - accuracy: 0.9375\n",
      "Epoch: 7 - loss: 0.1906 - accuracy: 0.9375\n",
      "Epoch: 8 - loss: 0.1809 - accuracy: 0.9375\n",
      "Epoch: 9 - loss: 0.1730 - accuracy: 0.9375\n",
      "Epoch: 10 - loss: 0.1666 - accuracy: 0.9375\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs): \n",
    "    for i in range((n-1)//bs + 1):\n",
    "        start_i = i * bs\n",
    "        end_i = start_i + bs\n",
    "        xb = x_train[start_i: end_i]\n",
    "        yb = y_train[start_i: end_i]\n",
    "        pred = model(xb)\n",
    "        loss = loss_fn(pred, yb)\n",
    "        acc = accuracy(pred, yb)\n",
    "        \n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "    \n",
    "    print(f\"Epoch: {epoch+1} - loss: {loss:.4f} - accuracy: {acc:.4f}\")        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Refactor using Dataset\n",
    "- PyTorch has an abstract Dataset class\n",
    "- A Dataset can be anything that has a `__len__` function (called by Python's standard len function) and a `__getitem__` function as a way of indexing into it.\n",
    "- PyTorch's TensorDataset is a Dataset wrapping tensors.\n",
    "- We can iterate, index, and slice along the first dimension of a tensor.\n",
    "- This will make it easier to access both the independent and dependent variables in the same line as we train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "# Both x_train and y_train can be combined in a single TensorDataset, \n",
    "# which will be easier to iterate over and slice.\n",
    "train_ds = TensorDataset(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, opt = get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 - loss: 0.3927 - accuracy: 0.9375\n",
      "Epoch: 2 - loss: 0.3070 - accuracy: 0.9375\n",
      "Epoch: 3 - loss: 0.2641 - accuracy: 0.9375\n",
      "Epoch: 4 - loss: 0.2359 - accuracy: 0.9375\n",
      "Epoch: 5 - loss: 0.2155 - accuracy: 0.9375\n",
      "Epoch: 6 - loss: 0.2001 - accuracy: 0.9375\n",
      "Epoch: 7 - loss: 0.1882 - accuracy: 0.9375\n",
      "Epoch: 8 - loss: 0.1787 - accuracy: 0.9375\n",
      "Epoch: 9 - loss: 0.1711 - accuracy: 0.9375\n",
      "Epoch: 10 - loss: 0.1650 - accuracy: 0.9375\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    for i in range((n-1)//bs + 1):\n",
    "        xb, yb = train_ds[i*bs: i*bs+bs]\n",
    "        pred = model(xb)\n",
    "        loss = loss_fn(pred, yb)\n",
    "        \n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "    print(f\"Epoch: {epoch+1} - loss: {loss:.4f} - accuracy: {acc:.4f}\")        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Refactor using DataLoader\n",
    "- Pytorch's DataLoader is responsible for managing batches\n",
    "- DataLoader makes it easier to iterate over batches\n",
    "- Rather than having to use `train_ds[i*bs : i*bs+bs]`, the DataLoader gives us each `minibatch automatically`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = TensorDataset(x_train, y_train)\n",
    "train_dl = DataLoader(train_ds, batch_size=bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 - loss: 0.1343 - accuracy: 0.9375\n",
      "Epoch: 2 - loss: 0.1329 - accuracy: 0.9375\n",
      "Epoch: 3 - loss: 0.1316 - accuracy: 0.9375\n",
      "Epoch: 4 - loss: 0.1304 - accuracy: 0.9375\n",
      "Epoch: 5 - loss: 0.1293 - accuracy: 0.9375\n",
      "Epoch: 6 - loss: 0.1282 - accuracy: 0.9375\n",
      "Epoch: 7 - loss: 0.1271 - accuracy: 0.9375\n",
      "Epoch: 8 - loss: 0.1261 - accuracy: 0.9375\n",
      "Epoch: 9 - loss: 0.1252 - accuracy: 0.9375\n",
      "Epoch: 10 - loss: 0.1243 - accuracy: 0.9375\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    for xb, yb in train_dl:\n",
    "        pred = model(xb)\n",
    "        loss = loss_fn(pred, yb)\n",
    "        \n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "        \n",
    "    print(f\"Epoch: {epoch+1} - loss: {loss:.4f} - accuracy: {acc:.4f}\")     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add validation\n",
    "- Shuffling the training data is important to prevent correlation between batches and overfitting\n",
    "- On the other hand, the validation loss will be identical whether we shuffle the validation set or not.\n",
    "- Since shuffling takes extra time, it makes no sense to shuffle the validation data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We'll use a batch size for the validation set that is twice as large as that for the training set.\n",
    "- This is because the validation set does not need backpropagation and thus takes less memory (it doesn't need to store the gradients)\n",
    "- We take advantage of this to use a larger batch size and compute the loss more quickly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = TensorDataset(x_train, y_train)\n",
    "train_dl = DataLoader(train_ds, batch_size=bs)\n",
    "\n",
    "valid_ds = TensorDataset(x_valid, y_valid)\n",
    "valid_dl = DataLoader(valid_ds, batch_size=bs*2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We will calculate and print the validation loss at the end of each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, opt = get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 - train loss: 0.1554 - val loss: 0.2773\n",
      "Epoch: 2 - train loss: 0.1519 - val loss: 0.2776\n",
      "Epoch: 3 - train loss: 0.1488 - val loss: 0.2780\n",
      "Epoch: 4 - train loss: 0.1460 - val loss: 0.2783\n",
      "Epoch: 5 - train loss: 0.1436 - val loss: 0.2786\n",
      "Epoch: 6 - train loss: 0.1414 - val loss: 0.2790\n",
      "Epoch: 7 - train loss: 0.1394 - val loss: 0.2793\n",
      "Epoch: 8 - train loss: 0.1376 - val loss: 0.2797\n",
      "Epoch: 9 - train loss: 0.1360 - val loss: 0.2800\n",
      "Epoch: 10 - train loss: 0.1345 - val loss: 0.2803\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    for xb, yb in train_dl:\n",
    "        pred = model(xb)\n",
    "        loss = loss_fn(pred, yb)\n",
    "        \n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "        \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        valid_loss = sum(loss_fn(model(xb), yb)\n",
    "                         for xb, yb in valid_dl)\n",
    "            \n",
    "    print(f\"Epoch: {epoch+1} - train loss: {loss:.4f} - val loss: {valid_loss/len(valid_dl):.4f}\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create fit() and get_data()\n",
    "- make caluating loss for training and validation set into one function call `loss_batch` (compute loss for one batch)\n",
    "- For training set we pass the optmizier to perform backprop and return loss\n",
    "- But for validation set we dont pass optimizer, just return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_batch(model, loss_fn, xb, yb, opt=None):\n",
    "    loss = loss_fn(model(xb), yb)\n",
    "    \n",
    "    if opt is not None:\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "    \n",
    "    return loss.item(), len(xb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def fit(epochs, model, loss_fn, opt, train_dl, valid_dl):\n",
    "    '''runs the necessary operations to train our model and \n",
    "    compute the training and validation losses for each epoch.\n",
    "    '''\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for xb, yb in train_dl: loss_batch(model, loss_fn, xb, yb, opt)\n",
    "            \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            losses, n = zip(*[loss_batch(model, loss_fn, xb, yb)\n",
    "                             for xb, yb in valid_dl])\n",
    "            \n",
    "        # Weighted average : sum(items * weights) / sum(weights)\\\n",
    "        # Final batch size will be smaller than bs\n",
    "        val_loss = np.sum(np.multiply(losses, n)) / np.sum(n)\n",
    "            \n",
    "        print(f'Epoch: {epoch+1} - val loss: {val_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(train_ds, valid_ds, bs):\n",
    "    '''returns dataloaders for`get_data` returns dataloaders for the training and validation sets. the training and validation sets.\n",
    "    '''\n",
    "    return (DataLoader(train_ds, batch_size=bs, shuffle=True),\n",
    "            DataLoader(valid_ds, batch_size=bs*2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 - val loss: 0.3511\n",
      "Epoch: 2 - val loss: 0.3084\n",
      "Epoch: 3 - val loss: 0.2950\n",
      "Epoch: 4 - val loss: 0.2934\n",
      "Epoch: 5 - val loss: 0.2811\n",
      "Epoch: 6 - val loss: 0.2766\n",
      "Epoch: 7 - val loss: 0.2769\n",
      "Epoch: 8 - val loss: 0.2763\n",
      "Epoch: 9 - val loss: 0.2704\n",
      "Epoch: 10 - val loss: 0.2760\n"
     ]
    }
   ],
   "source": [
    "# 3 lines of code to train a model\n",
    "train_dl, valid_dl = get_data(train_ds, valid_ds, bs)\n",
    "model, opt = get_model()\n",
    "fit(epochs, model, loss_fn, opt, train_dl, valid_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9219)"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xb, yb = next(iter(valid_dl))\n",
    "accuracy(model(xb), yb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Switch to CNN\n",
    "- Since none of the above funtions `loss_batch()` and `get_data()` assumed about the model - loosely coupled with model. We can still use those generic functions to train CNN model w/o any changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mnist_CNN(nn.Module): \n",
    "    def __init__(self): \n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, stride=2, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 16, kernel_size=3, stride=2, padding=1)\n",
    "        self.conv3 = nn.Conv2d(16, 10, kernel_size=3, stride=2, padding=1)\n",
    "        \n",
    "    def forward(self, xb):\n",
    "        xb = xb.view(-1, 1, 28, 28)\n",
    "        xb = F.relu(self.conv1(xb))\n",
    "        xb = F.relu(self.conv2(xb))\n",
    "        xb = F.relu(self.conv3(xb))\n",
    "        xb = F.avg_pool2d(xb, 4)\n",
    "        return xb.view(-1, xb.size(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr=0.1\n",
    "\n",
    "model = Mnist_CNN()\n",
    "\n",
    "# Using momentum on SGD, it will generally lead to faster training by taking previous update into account\n",
    "opt = optim.SGD(model.parameters(), lr=lr, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 - val loss: 0.3255\n",
      "Epoch: 2 - val loss: 0.2230\n",
      "Epoch: 3 - val loss: 0.1960\n",
      "Epoch: 4 - val loss: 0.2066\n",
      "Epoch: 5 - val loss: 0.1537\n",
      "Epoch: 6 - val loss: 0.1455\n",
      "Epoch: 7 - val loss: 0.1265\n",
      "Epoch: 8 - val loss: 0.1562\n",
      "Epoch: 9 - val loss: 0.1329\n",
      "Epoch: 10 - val loss: 0.1147\n"
     ]
    }
   ],
   "source": [
    "fit(epochs, model, loss_fn, opt, train_dl, valid_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9922)"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xb, yb = next(iter(valid_dl))\n",
    "accuracy(model(xb), yb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## nn.Sequential\n",
    "- A Sequential object runs each of the modules contained within it, in a sequential manner, makes it simpler to create neural network.\n",
    "- To take advantage of this, we need to be able to easily define a custom layer from a given function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lambda(nn.Module):\n",
    "    def __init__(self, func):\n",
    "        super().__init__()\n",
    "        self.func = func\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.func(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(x): return x.view(-1, 1, 28, 28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "    Lambda(preprocess),\n",
    "    nn.Conv2d(1, 16, kernel_size=3, stride=2, padding=1), nn.ReLU(),\n",
    "    nn.Conv2d(16, 16, kernel_size=3, stride=2, padding=1), nn.ReLU(),\n",
    "    nn.Conv2d(16, 10, kernel_size=3, stride=2, padding=1), nn.ReLU(),\n",
    "    nn.AvgPool2d(4),\n",
    "    Lambda(lambda x: x.view(x.size(0), -1))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = optim.SGD(model.parameters(), lr=lr, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 - val loss: 0.3378\n",
      "Epoch: 2 - val loss: 0.2794\n",
      "Epoch: 3 - val loss: 0.1971\n",
      "Epoch: 4 - val loss: 0.1867\n",
      "Epoch: 5 - val loss: 0.1890\n",
      "Epoch: 6 - val loss: 0.1584\n",
      "Epoch: 7 - val loss: 0.1506\n",
      "Epoch: 8 - val loss: 0.1386\n",
      "Epoch: 9 - val loss: 0.1599\n",
      "Epoch: 10 - val loss: 0.1379\n"
     ]
    }
   ],
   "source": [
    "fit(epochs, model, loss_fn, opt, train_dl, valid_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9766)"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xb, yb = next(iter(valid_dl))\n",
    "accuracy(model(xb), yb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrapping DataLoader\n",
    "- Our CNN model only works dor MNIST, since it assumes:\n",
    "    - input shape: 28 * 28\n",
    "    - final grid size is 4 * 4 ( that kernel size of avgpool we need to use to get 10 scores values)\n",
    "- Lets make our model works for any 2d image with single channel\n",
    "    1. we can remove the initial `Lambda layer` but moving the data preprocessing into a generator\n",
    "    1. replace `nn.AvgPool2d` with `nn.AdaptiveAvgPool2d`,  which allows us to define the size of the output tensor we want, rather than the input tensor we have. As a result, our model will work with any size input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(x, y): return x.view(-1, 1, 28, 28), y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WrapperDataLoader:\n",
    "    def __init__(self, dl, func):\n",
    "        self.dl = dl\n",
    "        self.func = func\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dl)\n",
    "    \n",
    "    def __iter__(self):\n",
    "        batches = iter(self.dl)\n",
    "        for b in batches:\n",
    "            yield self.func(*b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl, valid_dl = get_data(train_ds, valid_ds, bs)\n",
    "train_dl = WrapperDataLoader(train_dl, preprocess)\n",
    "valid_dl = WrapperDataLoader(valid_dl, preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Conv2d(1, 16, kernel_size=3, stride=2, padding=1), nn.ReLU(),\n",
    "    nn.Conv2d(16, 16, kernel_size=3, stride=2, padding=1), nn.ReLU(),\n",
    "    nn.Conv2d(16, 10, kernel_size=3, stride=2, padding=1), nn.ReLU(),\n",
    "    nn.AdaptiveAvgPool2d(1),\n",
    "    Lambda(lambda x: x.view(x.size(0), -1))\n",
    ")\n",
    "\n",
    "opt = optim.SGD(model.parameters(), lr=lr, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 - val loss: 0.3421\n",
      "Epoch: 2 - val loss: 0.2430\n",
      "Epoch: 3 - val loss: 0.2298\n",
      "Epoch: 4 - val loss: 0.2038\n",
      "Epoch: 5 - val loss: 0.1865\n",
      "Epoch: 6 - val loss: 0.1529\n",
      "Epoch: 7 - val loss: 0.1527\n",
      "Epoch: 8 - val loss: 0.1457\n",
      "Epoch: 9 - val loss: 0.1360\n",
      "Epoch: 10 - val loss: 0.1382\n"
     ]
    }
   ],
   "source": [
    "fit(epochs, model, loss_fn, opt, train_dl, valid_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([128, 1, 28, 28]), torch.Size([128]))"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xb, yb = next(iter(valid_dl))\n",
    "xb.size(), yb.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9922)"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(model(xb), yb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(x,y): return x.view(-1,1,28,28).to(dev),y.to(dev)\n",
    "\n",
    "train_dl,valid_dl = get_data(train_ds, valid_ds, bs)\n",
    "train_dl = WrapperDataLoader(train_dl, preprocess)\n",
    "valid_dl = WrapperDataLoader(valid_dl, preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(dev);\n",
    "opt = optim.SGD(model.parameters(), lr=lr, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 - val loss: 0.1254\n",
      "Epoch: 2 - val loss: 0.1341\n",
      "Epoch: 3 - val loss: 0.1276\n",
      "Epoch: 4 - val loss: 0.1270\n",
      "Epoch: 5 - val loss: 0.1208\n",
      "Epoch: 6 - val loss: 0.1220\n",
      "Epoch: 7 - val loss: 0.1306\n",
      "Epoch: 8 - val loss: 0.1174\n",
      "Epoch: 9 - val loss: 0.1213\n",
      "Epoch: 10 - val loss: 0.1104\n"
     ]
    }
   ],
   "source": [
    "fit(epochs, model, loss_fn, opt, train_dl, valid_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
